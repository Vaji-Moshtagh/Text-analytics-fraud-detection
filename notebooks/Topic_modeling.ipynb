{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for Fraud Detection\n",
    "\n",
    "This notebook demonstrates how to use Latent Dirichlet Allocation (LDA) for topic modeling to identify suspicious communication patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from text_cleaner import TextCleaner\n",
    "from topic_analyzer import TopicAnalyzer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Financial Communications Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger sample dataset with multiple topics\n",
    "communications = [\n",
    "    # Topic 1: Regular business operations\n",
    "    \"The quarterly review meeting is scheduled for next Monday.\",\n",
    "    \"Please submit your expense reports by the end of the month.\",\n",
    "    \"The new HR policy will be effective starting next quarter.\",\n",
    "    \"The office will be closed for maintenance this weekend.\",\n",
    "    \"We are updating our customer relationship management system.\",\n",
    "    \n",
    "    # Topic 2: Stock and trading\n",
    "    \"The stock price has increased by 15% this quarter.\",\n",
    "    \"Your stock options will vest next month as scheduled.\",\n",
    "    \"We should consider selling our company stock soon.\",\n",
    "    \"The trading window opens next week after the earnings call.\",\n",
    "    \"Our stock performance has been strong compared to competitors.\",\n",
    "    \n",
    "    # Topic 3: Financial results\n",
    "    \"The Q3 financial results exceeded our expectations.\",\n",
    "    \"Our revenue increased by 20% year over year.\",\n",
    "    \"We need to improve our profit margins in the coming quarters.\",\n",
    "    \"The financial audit is scheduled for next month.\",\n",
    "    \"Our EBITDA has improved significantly this quarter.\",\n",
    "    \n",
    "    # Topic 4: Potentially suspicious\n",
    "    \"We need to sell our shares before the announcement.\",\n",
    "    \"Let's move these transactions off the books temporarily.\",\n",
    "    \"Don't discuss the accounting irregularities with the auditors.\",\n",
    "    \"We should hide these losses until after the investor meeting.\",\n",
    "    \"The company will announce a major writedown next week.\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'content': communications})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean and Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text cleaner with custom stopwords\n",
    "custom_stopwords = ['please', 'would', 'could', 'should', 'will', 'next']\n",
    "cleaner = TextCleaner(custom_stopwords=custom_stopwords)\n",
    "\n",
    "# Clean the texts\n",
    "df['clean_content'] = df['content'].apply(cleaner.clean_text)\n",
    "\n",
    "# Tokenize the cleaned texts\n",
    "df['tokens'] = df['clean_content'].apply(lambda x: x.split())\n",
    "\n",
    "# Display results\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df[['content', 'clean_content', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize topic analyzer\n",
    "analyzer = TopicAnalyzer()\n",
    "\n",
    "# Prepare data for topic modeling\n",
    "analyzer.prepare_data(df['tokens'].tolist())\n",
    "\n",
    "# Build LDA model with 4 topics\n",
    "analyzer.build_model(num_topics=4, passes=10)\n",
    "\n",
    "# Print topics\n",
    "print(\"LDA Model Topics:\")\n",
    "for topic_id, terms in analyzer.lda_model.print_topics(num_words=8):\n",
    "    print(f\"Topic {topic_id}: {terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Topic Details for Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic details\n",
    "topic_details = analyzer.get_topic_details()\n",
    "\n",
    "# Display topic details\n",
    "topic_details.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine Topic Details with Original Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add document index to original dataframe\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns={'index': 'Document_Id'})\n",
    "\n",
    "# Merge topic details with original content\n",
    "combined_df = pd.merge(topic_details, df, on='Document_Id')\n",
    "\n",
    "# Display combined results\n",
    "combined_df[['Document_Id', 'Dominant_Topic', 'Topic_Prob', 'Topic_Keywords', 'content']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identify Suspicious Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze topic distributions\n",
    "topic_dist = combined_df['Dominant_Topic'].value_counts().reset_index()\n",
    "topic_dist.columns = ['Topic', 'Count']\n",
    "topic_dist['Percentage'] = topic_dist['Count'] / len(combined_df) * 100\n",
    "\n",
    "# Display topic distribution\n",
    "print(\"Topic Distribution:\")\n",
    "topic_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(topic_dist['Topic'].astype(str), topic_dist['Count'], color='skyblue')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('Distribution of Documents Across Topics')\n",
    "plt.xticks(topic_dist['Topic'].astype(str))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which topic seems suspicious\n",
    "print(\"Words in each topic:\")\n",
    "for topic_id, terms in analyzer.lda_model.print_topics(num_words=8):\n",
    "    print(f\"Topic {topic_id}: {terms}\")\n",
    "    \n",
    "# Identify suspicious topic based on keywords\n",
    "# Let's assume Topic X is suspicious based on keywords like 'sell', 'hide', 'before', etc.\n",
    "suspicious_topic = 3  # This would be determined by examining the topics\n",
    "\n",
    "print(f\"\\nBased on the keywords, Topic {suspicious_topic} appears to be potentially suspicious.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Flag Documents Associated with Suspicious Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag documents associated with suspicious topic\n",
    "combined_df = analyzer.flag_topic(combined_df, suspicious_topic)\n",
    "\n",
    "# Display flagged documents\n",
    "suspicious_docs = combined_df[combined_df['Flag'] == 1]\n",
    "print(f\"Found {len(suspicious_docs)} potentially suspicious documents:\")\n",
    "suspicious_docs[['Document_Id', 'Dominant_Topic', 'Topic_Prob', 'content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare our flagged documents with the known suspicious ones (the last 5 in our dataset)\n",
    "known_suspicious = df.iloc[-5:]['Document_Id'].tolist()\n",
    "model_flagged = suspicious_docs['Document_Id'].tolist()\n",
    "\n",
    "# Calculate performance metrics\n",
    "true_positives = len(set(known_suspicious) & set(model_flagged))\n",
    "false_positives = len(set(model_flagged) - set(known_suspicious))\n",
    "false_negatives = len(set(known_suspicious) - set(model_flagged))\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Performance Metrics:\")\n",
    "print(f\"- Precision: {precision:.2f}\")\n",
    "print(f\"- Recall: {recall:.2f}\")\n",
    "print(f\"- F1 Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary of Topic Modeling for Fraud Detection:\")\n",
    "print(f\"- Total documents analyzed: {len(df)}\")\n",
    "print(f\"- Number of topics identified: 4\")\n",
    "print(f\"- Suspicious topic identified: Topic {suspicious_topic}\")\n",
    "print(f\"- Potentially suspicious documents flagged: {len(suspicious_docs)}\")\n",
    "print(\"\\nConclusions:\")\n",
    "print(\"- Topic modeling can successfully identify clusters of suspicious communications\")\n",
    "print(\"- This approach can detect patterns that might be missed by simple keyword searches\")\n",
    "print(\"- The model could be improved with more data and fine-tuning of parameters\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"- Combine topic modeling with keyword-based approaches for better accuracy\")\n",
    "print(\"- Review flagged documents manually to confirm suspicions\")\n",
    "print(\"- Refine the model based on feedback from manual reviews\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
